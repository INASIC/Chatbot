{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot Documentation\n",
    "\n",
    "This project was heavily inspired by the series of [YouTube videos](https://www.youtube.com/watch?v=dvOnYLDg8_Y&t=20s) by [sentdex](https://www.youtube.com/channel/UCfzlCWGWYyIQ0aLC5w48gBQ). In this notebook, I present comprehensive documentation of my experience with this series, which involved creating a chatbot with deep learning, Python and TensorFlow.\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. Introduction & Collecting our Training Data\n",
    "2. Data structure\n",
    "3. Buffering dataset\n",
    "4. Determining insert\n",
    "5. Building database\n",
    "6. Database to training data\n",
    "7. Training a model\n",
    "8. NMT Concepts and Parameters\n",
    "9. Interacting with our Chatbot\n",
    "10. Further Work\n",
    "\n",
    "# Introduction & Collecting our Training Data\n",
    "\n",
    "The method used here in building this chatbot is designed to be generalizable for building chatbots that can be used for a diverse range of applications. A large differentiating factor between the different implementations of chatbots, will largely depend on the type of training data used to build the chatbot.  \n",
    "\n",
    "As with all machine learning applications, one of the biggest obstacles is to collect the relevant data, and manipulate it to be useful for a given task. One of the most common data sets that people tend to use for building (relatively weak) chatbots is the open-source [Cornell movie database](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html), which contains 220,000 conversational exchanges between 10,300 pairs of movie characters. In addition to it being open-source, it contains conversational data from different movies, different characters, and different genders, which allows for a dataset that is fairly balanced to train on. However, a major draw-back is that there is a fairly limited amount of conversational exchanges.\n",
    "\n",
    "In the era of deep learning, big data is really the fuel for our machine learning applications. As we are building a chatbot that utilizes deep learning, our model will necessarily be very data hungry. Therefore, the Cornell movie database discussed will be fairly limited in its size. In search of a larger corpus of conversational exchanges to train our chatbot, we thus turn to [Reddit](https://www.reddit.com/), a popular collection of online forums where people share news, stories, and other types of content, and users are encouraged to comment and discuss about everyones posts. The popularity of Reddit is immense. It was measured that almost 1.69 billion users had accessed the site in just the month of March 2018, alone ([source](https://www.statista.com/statistics/443332/reddit-monthly-visitors/)). \n",
    "\n",
    "To extract this conversational data from Reddit, we could have used the [Python Reddit API](https://praw.readthedocs.io/en/latest/), but it has some pretty strict limitations. This API unfortunately will not allow you to parse millions of rows without a longer period of time, or violating the Terms of Service. Instead we can use a Reddit post that provides a [data dump of 1.7 Billion Reddit Comments](https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/?st=j9udbxta&sh=69e4fee7). This datafile is about 250 GB of data, when compressed. \n",
    "\n",
    "I will compare the performance of the chatbot on different sizes, and receny of reddit comment data which will be as follows:\n",
    "1. A __small__ dataset of __1.4 GB__ collected from an __older__ time: __January 2012__\n",
    "2. A __medium__ sized dataset of __9.1 GB__ collected from a more __recent__ time, __June 2018__\n",
    "3. A __large__ dataset of ~ __250 GB__, from between __December 2005 to June 2018__\n",
    "\n",
    "This data was collected from this [website](http://files.pushshift.io/reddit/comments/), where the datafiles are available in compressed form - allowing for relatively fast download speeds.\n",
    "\n",
    "# Data Structure\n",
    "\n",
    "If you observe the following [sample Reddit post](https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/?sort=top) and scroll down to the comments, you can observe that the comments are arranged in a __tree-like structure__, where you have parent comments at the top, followed by child comments which are in response to the parent above it. What we will need to do is to pull these strings of comments apart, and then  pair them together in a parent-child (comment and reply) manner. This allows us to capture natural conversational exchanges between humans on the internet, which we can then provide as training data to our chatbot to mimic. \n",
    "\n",
    "In this section we begin by __building our database__ that will store our parent comments that are paired to their best child (reply) comments. The reason do this is because a lot of these files are way too big for us to read into RAM and create training files from. For now, to keep things relatively simple we will use __SQLite__ for our database. \n",
    "\n",
    "The datafiles we are using are stored in the __JSON__ format. Because of this, there is a lot of unnecessary data within each of the files. When building our database, we will extract the following data from our JSON files:\n",
    "* Comment score (karma)\n",
    "* The comment body itself\n",
    "* Subreddit\n",
    "* The parent_id\n",
    "* Time of creation (UTC)\n",
    "\n",
    "Let's begin building the database using Python and SQLite!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3  # For building our database\n",
    "import json  # To parse our datafiles\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeframe = '2012-01'  # Begin with our small dataset\n",
    "sql_transaction = []  # Efficiently parse rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SQL, you ideally want to have a __big SQL transaction__ when possible. This is because you don't want to (for example) handle millions of rows by inserting them one by one if you don't have to, because that can be incredibly really inefficient. Instead you want to build up a big transaction and then perform it all at once - as this will be much faster to execute.\n",
    "\n",
    "The code immediately below this writing will connect to, and create the database if it doesn't exist already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = sqlite3.connect('{}.db'.format(timeframe))  # Connects to database\n",
    "c = connection.cursor()  # Define cursor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, let's create a function that can help us store the parent_id, comment_id, the parent comment, the reply (comment), subreddit, the time, and then finally the score (votes) for the comments from the raw JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query function to extract data from raw JSON files\n",
    "def create_table():\n",
    "    c.execute(\"\"\"CREATE TABLE IF NOT EXISTS parent_reply(parent_id TEXT \n",
    "    PRIMARY KEY, comment_id TEXT UNIQUE, parent TEXT, comment TEXT, subreddit\n",
    "    TEXT, unix INT, score INT)\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the table, if it doesn't exist already:\n",
    "if __name__ == '__main__':\n",
    "    create_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having discussed the structure of our data, and having created our database to store that date - we will beging working through the data.\n",
    "\n",
    "# Buffering Data\n",
    "\n",
    "In this section we will begin iterating over our data files and store that information. \n",
    "\n",
    "Let us begin by first improving the above ```if``` statement to include some additional actions. We will record using ```row_counter``` how far we are in the file that we are iterating through. Additionally, we also record how many rows of our data are reply-comment pairs, which we will aim to use as our training data - using the ```paired_rows``` variable. Since the file is too large for us to deal with in memory, we will use the ```buffering``` parameter, so that we read the file in small chunks that we can easily work with, which is fine since all we care about is 1 row at a time.\n",
    "\n",
    "We will now read through this data row by row, which is of the JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    create_table()  # Creates the table, if it doesn't exist already\n",
    "    row_counter = 0  # Record how many rows we iterate through in our data\n",
    "    paired_rows = 0  # Record how many parent-child pairs we have found\n",
    "    \n",
    "    with open('../data/{}/RC_{}'.format(timeframe.split('-')[0], timeframe), \n",
    "              buffering=1000) as f:\n",
    "        for row in f:  # Extract relevant feature data from rows of json file \n",
    "            row_counter += 1\n",
    "            row = json.loads(row)\n",
    "            parent_id = row['parent_id']  \n",
    "            body = format_data(row['body'])\n",
    "            created_utc = row['created_utc']\n",
    "            score = row['score']\n",
    "            subreddit = row['subreddit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the ```format_data``` function call above. This is used to normalize the comments and to convert the newline character to a word. Let's create that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of any new lines, and convert the newline character to a word\n",
    "def format_data(data):\n",
    "    data = data.replace('\\n', ' newlinechar ').replace('\\r', ' newlinechar ').replace('\"\"', \"'\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can read the data into a Python object using ```json.loads()```, which simply takes a string formatted like a json object. \n",
    "\n",
    "All comments will initially not have a parent. This could be because it is either a top level comment (and the parent is the Reddit post itself), or because the parent isn't in the document. However, as we go through the document we will indeed find comments that do have parents within our database. When this occurs, we want to instead add this comment to the existing parent to store the conversation. Once we've gone through the file(s), we'll take the database and output our comment-reply pairs as training data, train the model and then finally have our completed chatbot. \n",
    "\n",
    "Before we input our data to the database, we should see if we can find the parent first. We achieve this with the ```find_parent``` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds the parent comment by the parent id (pid)\n",
    "def find_parent(pid):\n",
    "    try:\n",
    "        sql = \"SELECT comment FROM parent_reply WHERE comment_id = '{}' LIMIT 1\".format(pid)\n",
    "        c.execute(sql)\n",
    "        result = c.fetchone()\n",
    "        if result != None:\n",
    "            return result[0]\n",
    "        else:\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        #print('find_parent', e)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now improve our previous ```if``` statement by appending it with this newly created ```find_parent``` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    create_table()  # Creates the table, if it doesn't exist already\n",
    "    row_counter = 0  # Record how many rows we iterate through in our data\n",
    "    paired_rows = 0  # Record how many parent-child pairs we have found\n",
    "    \n",
    "    with open('../data/{}/RC_{}'.format(timeframe.split('-')[0], timeframe), \n",
    "              buffering=1000) as f:\n",
    "        for row in f:  # Extract relevant feature data from rows of json file \n",
    "            row_counter += 1\n",
    "            row = json.loads(row)\n",
    "            parent_id = row['parent_id']  \n",
    "            body = format_data(row['body'])\n",
    "            created_utc = row['created_utc']\n",
    "            score = row['score']\n",
    "            subreddit = row['subreddit']\n",
    "            parent_data = find_parent(parent_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determining Insert\n",
    "\n",
    "In this section we will begin building the logic required to determine whether or not to insert data, and how.\n",
    "\n",
    "To begin, we aim to impose a restriction on _all_ comments, regardless if there are any others. This is because we want to filter unhelpful comments, and instead focus our training data on the best, most upvoted comments (measured by their comment score: karma). For this reason, we will only consider comments with a comment score of 2 or higher. \n",
    "\n",
    "Here, we __require the comment score to be 2 or higher__, and also check if there's already an existing reply to the parent, and check its score. The significance of a comment score value of 2, is that it means that some other unique user saw this comment on Reddit and decided to upvote it - signalling it as a useful comment to others. However, the value 2 is somewhat arbitrary, and is a threshold parameter which can be changed for different applications. For example if we are dealing with a much larger dataset, with many more comments - then it may be helpful to set this threshold to be higher - for example ```score >= 15```. Regardless, for this application we will set it to 2 for now, and perhaps in future implementations, experiment between different threshold values and compare the chatbots performance. \n",
    "\n",
    "In addition, if there is an existing comment, and if our score is higher than the existing comment's score, we would like to replace it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    create_table()\n",
    "    row_counter = 0\n",
    "    paired_rows = 0\n",
    "\n",
    "    with open('J:/chatdata/reddit_data/{}/RC_{}'.format(timeframe.split('-')[0],timeframe), buffering=1000) as f:\n",
    "        for row in f:\n",
    "            row_counter += 1\n",
    "            row = json.loads(row)\n",
    "            parent_id = row['parent_id']\n",
    "            body = format_data(row['body'])\n",
    "            created_utc = row['created_utc']\n",
    "            score = row['score']\n",
    "            comment_id = row['name']\n",
    "            subreddit = row['subreddit']\n",
    "            parent_data = find_parent(parent_id)\n",
    "\n",
    "            if score >= 2:  # Impose threshold comment score\n",
    "                existing_comment_score = find_existing_score(parent_id)\n",
    "                if existing_comment_score:  \n",
    "                    if score > existing_comment_score:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note this requires the ```find_existing_score``` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_existing_score(pid):\n",
    "    try:\n",
    "        sql = \"SELECT score FROM parent_reply WHERE parent_id = '{}' LIMIT 1\".format(pid)\n",
    "        c.execute(sql)\n",
    "        result = c.fetchone()\n",
    "        if result != None:\n",
    "            return result[0]\n",
    "        else: return False\n",
    "    except Exception as e:\n",
    "        #print(str(e))\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be noted that on Reddit, many comments are either deleted or remove, but also some comments can be very long / very short. For our application, we want to make sure comments are of an acceptable length for training, and that the comment wasn'te removed or deleted. We achieve this using the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks if the comments are acceptable for training, i.e. they are of \n",
    "# sufficient length, and that they exist\n",
    "def acceptable(data):\n",
    "    if len(data.split(' ')) > 50 or len(data) < 1:\n",
    "        return False\n",
    "    elif len(data) > 1000:  # Ignore long comments\n",
    "        return False\n",
    "    elif data == '[deleted]':  # Ignore deleted comments\n",
    "        return False\n",
    "    elif data == '[removed]':  # Ignore removed comments\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
