{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot Documentation\n",
    "\n",
    "This project was heavily inspired by the series of [YouTube videos](https://www.youtube.com/watch?v=dvOnYLDg8_Y&t=20s) by [sentdex](https://www.youtube.com/channel/UCfzlCWGWYyIQ0aLC5w48gBQ). In this notebook, I present comprehensive documentation of my experience with this series, which involved creating a chatbot with deep learning, Python and TensorFlow.\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Introduction & Collecting our Training Data](#intro)\n",
    "2. [Data structure](#2)\n",
    "3. [Buffering dataset](#3)\n",
    "4. [Determining insert](#4)\n",
    "5. [Building database](#5)\n",
    "6. [Database to training data](#6)\n",
    "7. [Training a model](#7)\n",
    "8. NMT Concepts and Parameters\n",
    "9. Interacting with our Chatbot\n",
    "10. Further Work\n",
    "\n",
    "# <a id='intro'></a> Introduction & Collecting our Training Data\n",
    "\n",
    "The method used here in building this chatbot is designed to be generalizable for building chatbots that can be used for a diverse range of applications. A large differentiating factor between the different implementations of chatbots, will largely depend on the type of training data used to build the chatbot.  \n",
    "\n",
    "As with all machine learning applications, one of the biggest obstacles is to collect the relevant data, and manipulate it to be useful for a given task. One of the most common data sets that people tend to use for building (relatively weak) chatbots is the open-source [Cornell movie database](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html), which contains 220,000 conversational exchanges between 10,300 pairs of movie characters. In addition to it being open-source, it contains conversational data from different movies, different characters, and different genders, which allows for a dataset that is fairly balanced to train on. However, a major draw-back is that there is a fairly limited amount of conversational exchanges.\n",
    "\n",
    "In the era of deep learning, big data is really the fuel for our machine learning applications. As we are building a chatbot that utilizes deep learning, our model will necessarily be very data hungry. Therefore, the Cornell movie database discussed will be fairly limited in its size. In search of a larger corpus of conversational exchanges to train our chatbot, we thus turn to [Reddit](https://www.reddit.com/), a popular collection of online forums where people share news, stories, and other types of content, and users are encouraged to comment and discuss about everyones posts. The popularity of Reddit is immense. It was measured that almost 1.69 billion users had accessed the site in just the month of March 2018, alone ([source](https://www.statista.com/statistics/443332/reddit-monthly-visitors/)). \n",
    "\n",
    "To extract this conversational data from Reddit, we could have used the [Python Reddit API](https://praw.readthedocs.io/en/latest/), but it has some pretty strict limitations. This API unfortunately will not allow you to parse millions of rows without a longer period of time, or violating the Terms of Service. Instead we can use a Reddit post that provides a [data dump of 1.7 Billion Reddit Comments](https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/?st=j9udbxta&sh=69e4fee7). This datafile is about 250 GB of data, when compressed. \n",
    "\n",
    "I will compare the performance of the chatbot on different sizes, and receny of reddit comment data which will be as follows:\n",
    "1. A __small__ dataset of __1.4 GB__ collected from a __distant__ time: __January 2012__\n",
    "2. A __medium__ sized dataset of __9.1 GB__ collected from a more __recent__ time, __June 2018__\n",
    "3. A __large__ dataset of ~ __250 GB__, from between __December 2005 to June 2018__\n",
    "\n",
    "This data was collected from this <a id='downloadc'></a> [website](http://files.pushshift.io/reddit/comments/), where the datafiles are available in compressed form - allowing for relatively fast download speeds.\n",
    "\n",
    "# <a id='2'></a> Data Structure\n",
    "\n",
    "If you observe the following [sample Reddit post](https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/?sort=top) and scroll down to the comments, you can observe that the comments are arranged in a __tree-like structure__, where you have parent comments at the top, followed by child comments which are in response to the parent above it. What we will need to do is to pull these strings of comments apart, and then  pair them together in a parent-child (comment and reply) manner. This allows us to capture natural conversational exchanges between humans on the internet, which we can then provide as training data to our chatbot to mimic. \n",
    "\n",
    "In this section we begin by __building our database__ that will store our parent comments that are paired to their best child (reply) comments. The reason do this is because a lot of these files are way too big for us to read into RAM and create training files from. For now, to keep things relatively simple we will use __SQLite__ for our database. \n",
    "\n",
    "The datafiles we are using are stored in the __JSON__ format. Because of this, there is a lot of unnecessary data within each of the files. When building our database, we will extract the following data from our JSON files:\n",
    "* Comment score (karma)\n",
    "* The comment body itself\n",
    "* Subreddit\n",
    "* The parent_id\n",
    "* Time of creation (UTC)\n",
    "\n",
    "Let's begin building the database using Python and SQLite!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3  # For building our database\n",
    "import json  # To parse our datafiles\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeframe = '2012-01'  # Begin with our small dataset\n",
    "sql_transaction = []  # Efficiently parse rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SQL, you ideally want to have a __big SQL transaction__ when possible. This is because you don't want to (for example) handle millions of rows by inserting them one by one if you don't have to, because that can be incredibly really inefficient. Instead you want to build up a big transaction and then perform it all at once - as this will be much faster to execute.\n",
    "\n",
    "The code immediately below this writing will connect to, and create the database if it doesn't exist already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = sqlite3.connect('../data/{}.db'.format(timeframe))  # Connects to database\n",
    "c = connection.cursor()  # Define cursor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, let's create a function that can help us store the parent_id, comment_id, the parent comment, the reply (comment), subreddit, the time, and then finally the score (votes) for the comments from the raw JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query function to extract data from raw JSON files\n",
    "def create_table():\n",
    "    c.execute(\"\"\"CREATE TABLE IF NOT EXISTS parent_reply(parent_id TEXT \n",
    "    PRIMARY KEY, comment_id TEXT UNIQUE, parent TEXT, comment TEXT, subreddit\n",
    "    TEXT, unix INT, score INT)\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Creates the table, if it doesn't exist already:\n",
    "if __name__ == '__main__':\n",
    "    create_table()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having discussed the structure of our data, and having created our database to store that date - we will beging working through the data.\n",
    "\n",
    "# <a id='3'></a> Buffering Data\n",
    "\n",
    "In this section we will begin iterating over our data files and store that information. \n",
    "\n",
    "Let us begin by first improving the above ```if``` statement to include some additional actions. We will record using ```row_counter``` how far we are in the file that we are iterating through. Additionally, we also record how many rows of our data are reply-comment pairs, which we will aim to use as our training data - using the ```paired_rows``` variable. Since the file is too large for us to deal with in memory, we will use the ```buffering``` parameter, so that we read the file in small chunks that we can easily work with, which is fine since all we care about is 1 row at a time.\n",
    "\n",
    "We will now read through this data row by row, which is of the JSON format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "if __name__ == '__main__':\n",
    "    create_table()  # Creates the table, if it doesn't exist already\n",
    "    row_counter = 0  # Record how many rows we iterate through in our data\n",
    "    paired_rows = 0  # Record how many parent-child pairs we have found\n",
    "    \n",
    "    with open('../data/{}/RC_{}'.format(timeframe.split('-')[0], timeframe), \n",
    "              buffering=1000) as f:\n",
    "        for row in f:  # Extract relevant feature data from rows of json file \n",
    "            row_counter += 1\n",
    "            row = json.loads(row)\n",
    "            parent_id = row['parent_id']  \n",
    "            body = format_data(row['body'])\n",
    "            created_utc = row['created_utc']\n",
    "            score = row['score']\n",
    "            subreddit = row['subreddit']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the ```format_data``` function call above. This is used to normalize the comments and to convert the newline character to a word. Let's create that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of any new lines, and convert the newline character to a word\n",
    "def format_data(data):\n",
    "    data = data.replace('\\n', ' newlinechar ').replace('\\r', ' newlinechar ').replace('\"\"', \"'\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can read the data into a Python object using ```json.loads()```, which simply takes a string formatted like a json object. \n",
    "\n",
    "All comments will initially not have a parent. This could be because it is either a top level comment (and the parent is the Reddit post itself), or because the parent isn't in the document. However, as we go through the document we will indeed find comments that do have parents within our database. When this occurs, we want to instead add this comment to the existing parent to store the conversation. Once we've gone through the file(s), we'll take the database and output our comment-reply pairs as training data, train the model and then finally have our completed chatbot. \n",
    "\n",
    "Before we input our data to the database, we should see if we can find the parent first. We achieve this with the ```find_parent``` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds the parent comment by the parent id (pid)\n",
    "def find_parent(pid):\n",
    "    try:\n",
    "        sql = \"SELECT comment FROM parent_reply WHERE comment_id = '{}' LIMIT 1\".format(pid)\n",
    "        c.execute(sql)\n",
    "        result = c.fetchone()\n",
    "        if result != None:\n",
    "            return result[0]\n",
    "        else:\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        #print('find_parent', e)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now improve our previous ```if``` statement by appending it with this newly created ```find_parent``` function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "if __name__ == '__main__':\n",
    "    create_table()  # Creates the table, if it doesn't exist already\n",
    "    row_counter = 0  # Record how many rows we iterate through in our data\n",
    "    paired_rows = 0  # Record how many parent-child pairs we have found\n",
    "    \n",
    "    with open('../data/{}/RC_{}'.format(timeframe.split('-')[0], timeframe), \n",
    "              buffering=1000) as f:\n",
    "        for row in f:  # Extract relevant feature data from rows of json file \n",
    "            row_counter += 1\n",
    "            row = json.loads(row)\n",
    "            parent_id = row['parent_id']  \n",
    "            body = format_data(row['body'])\n",
    "            created_utc = row['created_utc']\n",
    "            score = row['score']\n",
    "            subreddit = row['subreddit']\n",
    "            parent_data = find_parent(parent_id)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='4'></a> Determining Insert\n",
    "\n",
    "In this section we will begin building the logic required to determine whether or not to insert data, and how.\n",
    "\n",
    "To begin, we aim to impose a restriction on _all_ comments, regardless if there are any others. This is because we want to filter unhelpful comments, and instead focus our training data on the best, most upvoted comments (measured by their comment score: karma). For this reason, we will only consider comments with a comment score of 2 or higher. \n",
    "\n",
    "Here, we __require the comment score to be 2 or higher__, and also check if there's already an existing reply to the parent, and check its score. The significance of a comment score value of 2, is that it means that some other unique user saw this comment on Reddit and decided to upvote it - signalling it as a useful comment to others. However, the value 2 is somewhat arbitrary, and is a threshold parameter which can be changed for different applications. For example if we are dealing with a much larger dataset, with many more comments - then it may be helpful to set this threshold to be higher - for example ```score >= 15```. Regardless, for this application we will set it to 2 for now, and perhaps in future implementations, experiment between different threshold values and compare the chatbots performance. \n",
    "\n",
    "In addition, if there is an existing comment, and if our score is higher than the existing comment's score, we would like to replace it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "if __name__ == '__main__':\n",
    "    create_table()\n",
    "    row_counter = 0\n",
    "    paired_rows = 0\n",
    "\n",
    "    with open('../data/{}/RC_{}'.format(timeframe.split('-')[0],timeframe), buffering=1000) as f:\n",
    "        for row in f:\n",
    "            row_counter += 1\n",
    "            row = json.loads(row)\n",
    "            parent_id = row['parent_id']\n",
    "            body = format_data(row['body'])\n",
    "            created_utc = row['created_utc']\n",
    "            score = row['score']\n",
    "            comment_id = row['name']\n",
    "            subreddit = row['subreddit']\n",
    "            parent_data = find_parent(parent_id)\n",
    "\n",
    "            if score >= 2:  # Impose threshold comment score\n",
    "                existing_comment_score = find_existing_score(parent_id)\n",
    "                if existing_comment_score:  \n",
    "                    if score > existing_comment_score:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note this requires the ```find_existing_score``` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_existing_score(pid):\n",
    "    try:\n",
    "        sql = \"SELECT score FROM parent_reply WHERE parent_id = '{}' LIMIT 1\".format(pid)\n",
    "        c.execute(sql)\n",
    "        result = c.fetchone()\n",
    "        if result != None:\n",
    "            return result[0]\n",
    "        else: return False\n",
    "    except Exception as e:\n",
    "        #print(str(e))\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be noted that on Reddit, many comments are either deleted or removed, but also some comments can be very long / very short. For our application, we want to make sure comments are of an acceptable length for training, and that the comment wasn't removed or deleted. We achieve this using the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks if the comments are acceptable for training, i.e. they are of \n",
    "# sufficient length, and that they exist\n",
    "def acceptable(data):\n",
    "    if len(data.split(' ')) > 50 or len(data) < 1:  # Ignore long/short comments\n",
    "        return False\n",
    "    elif len(data) > 1000:  # Ignore long comments\n",
    "        return False\n",
    "    elif data == '[deleted]':  # Ignore deleted comments\n",
    "        return False\n",
    "    elif data == '[removed]':  # Ignore removed comments\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='5'></a> Building Database\n",
    "\n",
    "Up to this point we have been working with our data and preparing the logic for how we want to insert it into a database. In this section, we will begin actually building the database.\n",
    "\n",
    "First, we will improve our previous ```if``` statement with our newly created ```acceptable``` function. In addition, we will add a code block that use SQL to insert and replace the comment to the database, if the score is above the threshold, the comment is acceptable, and if the score is above the parent comment's score. We achieve this with the ```sql_insert_replace``` function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "if __name__ == '__main__':\n",
    "    create_table()\n",
    "    row_counter = 0\n",
    "    paired_rows = 0\n",
    "\n",
    "    with open('../data/{}/RC_{}'.format(timeframe.split('-')[0],timeframe), buffering=1000) as f:\n",
    "        for row in f:\n",
    "            row_counter += 1\n",
    "            row = json.loads(row)\n",
    "            parent_id = row['parent_id']\n",
    "            body = format_data(row['body'])\n",
    "            created_utc = row['created_utc']\n",
    "            score = row['score']\n",
    "            comment_id = row['name']\n",
    "            subreddit = row['subreddit']\n",
    "            parent_data = find_parent(parent_id)\n",
    "\n",
    "            if score >= 2:  # Impose threshold comment score\n",
    "                if acceptable(body):  # Check if comment body is suitable \n",
    "                    existing_comment_score = find_existing_score(parent_id)\n",
    "                    if existing_comment_score:  \n",
    "                        if score > existing_comment_score:\n",
    "                            sql_insert_replace_comment(comment_id, parent_id, parent_data, body, subreddit, created_utc, score)\n",
    "                    else:  # If there is no existing comment score\n",
    "                        if parent_data:\n",
    "                            sql_insert_has_parent(comment_id, parent_id, parent_data, body, subreddit, created_utc, score)\n",
    "                        else:\n",
    "                            sql_insert_no_parent(comment_id, parent_id, body, subreddit, created_utc, score)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice here we have 3 additional function calls that do not exist yet. Let's create them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwrites the parent_id comment with the current comment, if it has a better score\n",
    "def sql_insert_replace_comment(comment_id, parent_id, parent, comment, subreddit, created_utc, score):\n",
    "    try:\n",
    "        sql = \"\"\"UPDATE parent_reply SET parent_id = ?, comment_id = ?, parent = ?, comment = ?, subreddit = ?, unix = ?, score = ? WHERE parent_id =?;\"\"\".format(parent_id, comment_id, parent, comment, subreddit, int(created_utc), score, parent_id)\n",
    "        transaction_bldr(sql)\n",
    "    except Exception as e:\n",
    "        print('s-UPDATE insertion',str(e))\n",
    "        \n",
    "        \n",
    "# Inserts commment at the parent_id, if we had a comment body for that parent  \n",
    "def sql_insert_has_parent(comment_id, parent_id, parent, comment, subreddit, created_utc, score):\n",
    "    try:\n",
    "        sql = \"\"\"INSERT INTO parent_reply (parent_id, comment_id, parent, comment, subreddit, unix, score) VALUES (\"{}\",\"{}\",\"{}\",\"{}\",\"{}\",{},{});\"\"\".format(parent_id, comment_id, parent, comment, subreddit, int(created_utc), score)\n",
    "        transaction_bldr(sql)\n",
    "    except Exception as e:\n",
    "        print('s-PARENT insertion',str(e))\n",
    "        \n",
    "        \n",
    "# Inserts comment if there was no parent, but we still want the parent_id \n",
    "def sql_insert_no_parent(comment_id, parent_id, comment, subreddit, time, score):\n",
    "    try:\n",
    "        sql = \"\"\"INSERT INTO parent_reply (parent_id, comment_id, comment, subreddit, unix, score) VALUES (\"{}\",\"{}\",\"{}\",\"{}\",{},{});\"\"\".format(parent_id, comment_id, comment, subreddit, int(created_utc), score)\n",
    "        transaction_bldr(sql)\n",
    "    except Exception as e:\n",
    "        print('s-NO_PARENT insertion',str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the last function we will need to define to create this database will be the ```transaction_bldr```. This will build up insertion statements and commit them in groups, rather than one-by-one. This will help make our code to be much more efficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficiently builds up inerstion statements, and commits them in groups\n",
    "def transaction_bldr(sql):\n",
    "    global sql_transaction\n",
    "    sql_transaction.append(sql)\n",
    "    if len(sql_transaction) > 1000:\n",
    "        c.execute('BEGIN TRANSACTION')\n",
    "        for s in sql_transaction: \n",
    "            try:\n",
    "                c.execute(s)\n",
    "            except:\n",
    "                pass\n",
    "        connection.commit()\n",
    "        sql_transaction = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us add some additional lines of code, that allow us to see where we are during our iteration - so we'll output every 100,000 rows of data some information. Having done this, let's finally run our big ```if``` statement, and build the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the database using SQLite\n",
    "if __name__ == '__main__':\n",
    "    create_table()\n",
    "    row_counter = 0\n",
    "    paired_rows = 0\n",
    "\n",
    "    with open('../data/{}/RC_{}'.format(timeframe.split('-')[0],timeframe), buffering=1000) as f:\n",
    "        for row in f:\n",
    "            row_counter += 1\n",
    "            row = json.loads(row)\n",
    "            parent_id = row['parent_id']\n",
    "            body = format_data(row['body'])\n",
    "            created_utc = row['created_utc']\n",
    "            score = row['score']\n",
    "            comment_id = row['name']\n",
    "            subreddit = row['subreddit']\n",
    "            parent_data = find_parent(parent_id)\n",
    "            if score >= 2:\n",
    "                existing_comment_score = find_existing_score(parent_id)\n",
    "                if existing_comment_score:\n",
    "                    if score > existing_comment_score:\n",
    "                        if acceptable(body):\n",
    "                            sql_insert_replace_comment(comment_id,parent_id,parent_data,body,subreddit,created_utc,score)\n",
    "                else:\n",
    "                    if acceptable(body):\n",
    "                        if parent_data:\n",
    "                            sql_insert_has_parent(comment_id,parent_id,parent_data,body,subreddit,created_utc,score)\n",
    "                            paired_rows += 1\n",
    "                        else:\n",
    "                            sql_insert_no_parent(comment_id,parent_id,body,subreddit,created_utc,score)\n",
    "                            \n",
    "            if row_counter % 100000 == 0:  # Inform us our progress\n",
    "                print('Total Rows Read: {}, Paired Rows: {}, Time: {}'.format(row_counter, paired_rows, str(datetime.now())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having completed constructing the database using SQLite, we can then view the created database within the ```data``` directory, which is located one directory above this ```Documentation.ipynb```. The created database has the file extension ```.db```. For those that are curious, we can open and view this created database using the open-source tool [Database Browser for SQLite](https://sqlitebrowser.org/). I have decided not to commit this database to GitHub, as it is extremely large in size. However, running this iPython notebook yourself (after downloading the compressed files [discussed earlier](#downloadc)) will allow you to create the database for yourself, if you desire.\n",
    "\n",
    "# <a id='6'></a> Database to Training Data\n",
    "\n",
    "Having created our database containing pairs of Reddit comments and replies, we will now  generate training data from it. This training data will later be used to train our models of chatbots.\n",
    "\n",
    "The model we will be building with [TensorFlow](https://www.tensorflow.org/) is a type of [Recurrent Neural Network](https://www.tensorflow.org/tutorials/sequences/recurrent) (RNN) known as a [sequence-to-sequence](https://www.tensorflow.org/versions/r1.2/tutorials/seq2seq) model.\n",
    "\n",
    "What we first want to do is create two files: a parent-comment file, and then a reply file - where each row corresponds to the comment in the opposite file. We create these files as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 rows completed so far\n",
      "200000 rows completed so far\n",
      "300000 rows completed so far\n",
      "400000 rows completed so far\n",
      "500000 rows completed so far\n",
      "600000 rows completed so far\n",
      "700000 rows completed so far\n"
     ]
    }
   ],
   "source": [
    "# Grabs comment pairs from database and append them to their respective training files \n",
    "timeframes = ['2012-01']  # Begin with just our small dataset\n",
    "\n",
    "for timeframe in timeframes:\n",
    "    connection = sqlite3.connect('../data/{}.db'.format(timeframe))\n",
    "    c = connection.cursor()\n",
    "    limit = 5000  # Determines how much we'll pull at a time into our pandas DF\n",
    "    last_unix = 0  # Helps us buffer through the database\n",
    "    cur_length = limit  \n",
    "    counter = 0\n",
    "    test_done = False\n",
    "    \n",
    "    # Continue pulling data into our dataframe, while cur_length == limit\n",
    "    while cur_length == limit:\n",
    "        df = pd.read_sql(\"SELECT * FROM parent_reply WHERE unix > {} AND parent NOT NULL AND score > 0 ORDER BY unix ASC LIMIT {}\".format(last_unix, limit), connection)\n",
    "        last_unix = df.tail(1)['unix'].values[0]\n",
    "        cur_length = len(df)\n",
    "        \n",
    "        if not test_done:\n",
    "            with open('../data/test.from', 'a', encoding='utf8') as f:\n",
    "                for content in df['parent'].values:\n",
    "                    f.write(content+'\\n')\n",
    "            with open('../data/test.to', 'a', encoding='utf8') as f:\n",
    "                for content in df['comment'].values:\n",
    "                    f.write(content+'\\n')\n",
    "            test_done = True\n",
    "        else:\n",
    "            with open('../data/train.from','a', encoding='utf8') as f:\n",
    "                for content in df['parent'].values:\n",
    "                    f.write(content+'\\n')\n",
    "\n",
    "            with open('../data/train.to','a', encoding='utf8') as f:\n",
    "                for content in df['comment'].values:\n",
    "                    f.write(str(content)+'\\n')\n",
    "                    \n",
    "        counter += 1  # Track progress\n",
    "        if counter % 20 == 0:\n",
    "            print(counter*limit,'rows completed so far')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having created our training data from our database, we will now use it to train a model.\n",
    "\n",
    "# <a id='7'></a>Training a Model\n",
    "\n",
    "There are endless models that we could choose from, and adapt to our needs. In this case, we will be using [sequence-to-sequence models](https://www.tensorflow.org/versions/r1.2/tutorials/seq2seq), since they can be used for a wide variety of applications (not just chatbots). The versatility of these models stems from the perspective that (in general) everything in life can be reduced to sequences being mapped to other sequences. This is of interest, because the knowledge we have learned in this project can be applied to train many other models for different types of applications in the future. However, before we get ahead of ourselves, let us return to the scope of this project: building a chatbot.\n",
    "\n",
    "TensorFlow offers an excellent [Neural Machine Translation (NMT) tutorial](https://github.com/tensorflow/nmt) that uses the latest version of [seq2seq](https://www.tensorflow.org/versions/r1.2/tutorials/seq2seq). Here, we will be building upon the set of utilities built on top of [TensorFlow's NMT code](https://github.com/tensorflow/nmt), by following another project by [Daniel Kukiela](https://github.com/daniel-kukiela): [NMT Chatbot](https://github.com/daniel-kukiela/nmt-chatbot).\n",
    "\n",
    "Since my laptop is not powerful enough to train complex models in a  reasonable time, I will instead be using [Paperspace](https://www.paperspace.com) as a cloud computing service to train the models. I should note that other cloud computing services exist, such as [Amazon Web Services (AWS)](https://aws.amazon.com/),  [Google Cloud Platform (GCP)](https://cloud.google.com/), and [Microsoft Azure](https://azure.microsoft.com/en-us/) - however for this project I will be using [Paperspace](https://www.paperspace.com). \n",
    "\n",
    "Once you are in your Virtual Machine (VM) environment (or if you choose to train the chatbot locally), run the following lines of code in a Bash terminal to start training a sample chatbot:\n",
    "\n",
    "1. ```$ git clone --recursive https://github.com/daniel-kukiela/nmt-chatbot```\n",
    "(or)\n",
    "```$ git clone --branch v0.1 --recursive https://github.com/daniel-kukiela/nmt-chatbot.git``` (for a version featured in Sentdex tutorial)\n",
    "2. ```$ cd nmt-chatbot```\n",
    "3. ```$ pip install -r requirements.txt``` TensorFlow-GPU is one of the requirements. You also need CUDA Toolkit 8.0 and cuDNN 6.1. (Windows tutorial: https://www.youtube.com/watch?v=r7-WPbx8VuY Linux tutorial: https://pythonprogramming.net/how-to-cuda-gpu-tensorflow-deep-learning-tutorial/)\n",
    "4. ```$ cd setup```\n",
    "5. (optional) edit settings.py to your liking. These are a decent starting point for ~4GB of VRAM, you should first start by trying to raise vocab if you can.\n",
    "6. (optional) Edit text files containing rules in the setup directory.\n",
    "7. Place training data inside \"new_data\" folder (train.(from|to), tst2013.(from|to), tst2013(from|to)). We have provided some sample data for those who just want to do a quick test drive.\n",
    "8. ```$ python prepare_data.py``` ...Run setup/prepare_data.py - a new folder called \"data\" will be created with prepared training data\n",
    "9. ```$ cd ../```\n",
    "10. ```$ python train.py``` ```  Begin training\n",
    "\n",
    "# NMT Concepts and Parameters\n",
    "\n",
    "NMT stands for ___Neural Machine Translation___."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=gFxiQXnt9w4&list=PLQVvvaa0QuDdc2k5dwtDTyT9aCja0on8j&index=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
