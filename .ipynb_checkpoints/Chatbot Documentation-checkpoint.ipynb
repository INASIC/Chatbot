{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot Documentation\n",
    "\n",
    "This project was heavily inspired by the series of [YouTube videos](https://www.youtube.com/watch?v=dvOnYLDg8_Y&t=20s) by [sentdex](https://www.youtube.com/channel/UCfzlCWGWYyIQ0aLC5w48gBQ). In this notebook, I present comprehensive documentation of my experience with this series, which involved creating a chatbot with deep learning, Python and TensorFlow.\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. Introduction\n",
    "2. Data structure\n",
    "3. Buffering dataset\n",
    "4. Determining insert\n",
    "5. Building database\n",
    "6. Database to training data\n",
    "7. Training a model\n",
    "8. NMT Concepts and Parameters\n",
    "9. Interacting with our Chatbot\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The method used here in building this chatbot is designed to be generalizable for building chatbots that can be used for a diverse range of applications. A large differentiating factor between the different implementations of chatbots, will largely be the type of training data used to build the chatbot.  \n",
    "\n",
    "As with all machine learning applications, one of the biggest obstacles is to collect the relevant data, and manipulate it to be useful for a given task. One of the most common data sets that people tend to use for building (relatively weak) chatbots is the open-source [Cornell movie database](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html), which contains 220,000 conversational exchanges between 10,300 pairs of movie characters. In addition to it being open-source, it contains conversational data from different movies, different characters, and different genders, which allows for a dataset that is fairly balanced to train on. However, a major draw-back is that there is a fairly limited amount of conversational exchanges.\n",
    "\n",
    "In the era of deep learning, big data is really the fuel for our machine learning applications. As we are building a chatbot that utilizes deep learning, our model will necessarily be very data hungry. Therefore, the Cornell movie database discussed will be fairly limited in its size. In search of a larger corpus of conversational exchanges to train our chatbot, we thus turn to [Reddit](https://www.reddit.com/), a popular collection of online forums where people share news, stories, and other types of content, and users are encouraged to comment and discuss about everyones posts. The popularity of Reddit is immense. It was measured that almost 1.69 billion users had accessed the site in just the month of March 2018, alone ([source](https://www.statista.com/statistics/443332/reddit-monthly-visitors/)). \n",
    "\n",
    "To extract this conversational data from Reddit, we could have used the [Python Reddit API](https://praw.readthedocs.io/en/latest/), but it has some pretty strict limitations. This API unfortunately will not allow you to parse millions of rows without a longer period of time, or violating the Terms of Service. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
