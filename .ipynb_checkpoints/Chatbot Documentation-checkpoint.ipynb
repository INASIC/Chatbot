{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot Documentation\n",
    "\n",
    "This project was heavily inspired by the series of [YouTube videos](https://www.youtube.com/watch?v=dvOnYLDg8_Y&t=20s) by [sentdex](https://www.youtube.com/channel/UCfzlCWGWYyIQ0aLC5w48gBQ). In this notebook, I present comprehensive documentation of my experience with this series, which involved creating a chatbot with deep learning, Python and TensorFlow.\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. Introduction & Collecting our Training Data\n",
    "2. Data structure\n",
    "3. Buffering dataset\n",
    "4. Determining insert\n",
    "5. Building database\n",
    "6. Database to training data\n",
    "7. Training a model\n",
    "8. NMT Concepts and Parameters\n",
    "9. Interacting with our Chatbot\n",
    "\n",
    "# Introduction & Collecting our Training Data\n",
    "\n",
    "The method used here in building this chatbot is designed to be generalizable for building chatbots that can be used for a diverse range of applications. A large differentiating factor between the different implementations of chatbots, will largely depend on the type of training data used to build the chatbot.  \n",
    "\n",
    "As with all machine learning applications, one of the biggest obstacles is to collect the relevant data, and manipulate it to be useful for a given task. One of the most common data sets that people tend to use for building (relatively weak) chatbots is the open-source [Cornell movie database](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html), which contains 220,000 conversational exchanges between 10,300 pairs of movie characters. In addition to it being open-source, it contains conversational data from different movies, different characters, and different genders, which allows for a dataset that is fairly balanced to train on. However, a major draw-back is that there is a fairly limited amount of conversational exchanges.\n",
    "\n",
    "In the era of deep learning, big data is really the fuel for our machine learning applications. As we are building a chatbot that utilizes deep learning, our model will necessarily be very data hungry. Therefore, the Cornell movie database discussed will be fairly limited in its size. In search of a larger corpus of conversational exchanges to train our chatbot, we thus turn to [Reddit](https://www.reddit.com/), a popular collection of online forums where people share news, stories, and other types of content, and users are encouraged to comment and discuss about everyones posts. The popularity of Reddit is immense. It was measured that almost 1.69 billion users had accessed the site in just the month of March 2018, alone ([source](https://www.statista.com/statistics/443332/reddit-monthly-visitors/)). \n",
    "\n",
    "To extract this conversational data from Reddit, we could have used the [Python Reddit API](https://praw.readthedocs.io/en/latest/), but it has some pretty strict limitations. This API unfortunately will not allow you to parse millions of rows without a longer period of time, or violating the Terms of Service. Instead we can use a Reddit post that provides a [data dump of 1.7 Billion Reddit Comments](https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/?st=j9udbxta&sh=69e4fee7). This datafile is about 250 GB of data, when compressed. \n",
    "\n",
    "I will compare the performance of the chatbot on different sizes, and receny of reddit comment data which will be as follows:\n",
    "1. A __small__ dataset of __1.4 GB__ collected from an __older__ time: __January 2012__\n",
    "2. A __medium__ sized dataset of __9.1 GB__ collected from a more __recent__ time, __June 2018__\n",
    "3. A __large__ dataset of ~ __250 GB__, from between __December 2005 to June 2018__\n",
    "\n",
    "This data was collected from this [website](http://files.pushshift.io/reddit/comments/), where the datafiles are available in compressed form - allowing for relatively fast download speeds.\n",
    "\n",
    "# Data Structure\n",
    "\n",
    "If you observe the following [sample Reddit post](https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/?sort=top) and scroll down to the comments, you can observe that the comments are arranged in a __tree-like structure__, where you have parent comments at the top, followed by child comments which are in response to the parent above it. What we will need to do is to pull these strings of comments apart, and then  pair them together in a parent-child (comment and reply) manner. This allows us to capture natural conversational exchanges between humans on the internet, which we can then provide as training data to our chatbot to mimic. \n",
    "\n",
    "In this section we begin by __building our database__ that will store our parent comments that are paired to their best child (reply) comments. The reason do this is because a lot of these files are way too big for us to read into RAM and create training files from. For now, to keep things relatively simple we will use __SQLite__ for our database. \n",
    "\n",
    "The datafiles we are using are stored in the __JSON__ format. Because of this, there is a lot of unnecessary data within each of the files. When building our database, we will extract the following data from our JSON files:\n",
    "* Comment score (karma)\n",
    "* The comment itself\n",
    "* ...\n",
    "\n",
    "Let's begin building that database using Python and SQLite!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3  # For building our database\n",
    "import json  # To parse our datafiles\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeframe = '2012-01'  # Begin with our small dataset\n",
    "sql_transaction = []  # Efficiently parse rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SQL, you ideally want to have a __big SQL transaction__ when possible. This is because you don't want to (for example) handle millions of rows by inserting them one by one if you don't have to, because that can be incredibly really inefficient. Instead you want to build up a big transaction and then perform it all at once - as this will be much faster to execute.\n",
    "\n",
    "The code immediately below this writing will connect to, and create the database if it doesn't exist already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = sqlite3.connect('{}.db'.format(timeframe))  # Connects to database\n",
    "c = connection.cursor()  # Define cursor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, let's create a function that can help us store the parent_id, comment_id, the parent comment, the reply (comment), subreddit, the time, and then finally the score (votes) for the comments from the raw JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query function to extract data from raw JSON files\n",
    "def create_table():\n",
    "    c.execute(\"\"\"CREATE TABLE IF NOT EXISTS parent_reply(parent_id TEXT \n",
    "    PRIMARY KEY, comment_id TEXT UNIQUE, parent TEXT, comment TEXT, subreddit\n",
    "    TEXT, unix INT, score INT)\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the table, if it doesn't exist already:\n",
    "if __name__ == '__main__':\n",
    "    create_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buffering Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
